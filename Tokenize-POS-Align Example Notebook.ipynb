{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f825919",
   "metadata": {},
   "source": [
    "## IMPORTS AND STANZA MODEL DOWNLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21801ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1219b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdef0c2dd504419e9b65ec674110b8c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-15 13:44:22 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-06-15 13:44:23 INFO: File exists: /home/kapilrk04/stanza_resources/en/default.zip\n",
      "2023-06-15 13:44:26 INFO: Finished downloading models and saved to /home/kapilrk04/stanza_resources.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e1982781f64fe5b532f04cf90ae199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-15 13:44:26 INFO: Downloading default packages for language: hi (Hindi) ...\n",
      "2023-06-15 13:44:27 INFO: File exists: /home/kapilrk04/stanza_resources/hi/default.zip\n",
      "2023-06-15 13:44:28 INFO: Finished downloading models and saved to /home/kapilrk04/stanza_resources.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b93728e1ba541e28b028966bc53592c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-15 13:44:28 INFO: Downloading default packages for language: mr (Marathi) ...\n",
      "2023-06-15 13:44:32 INFO: File exists: /home/kapilrk04/stanza_resources/mr/default.zip\n",
      "2023-06-15 13:44:35 INFO: Finished downloading models and saved to /home/kapilrk04/stanza_resources.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a190b4d0973a4409ba6adad8368c4282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-15 13:44:35 INFO: Downloading default packages for language: ta (Tamil) ...\n",
      "2023-06-15 13:44:36 INFO: File exists: /home/kapilrk04/stanza_resources/ta/default.zip\n",
      "2023-06-15 13:44:37 INFO: Finished downloading models and saved to /home/kapilrk04/stanza_resources.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f114ec872d844897ba3d81b173480bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-15 13:44:37 INFO: Downloading default packages for language: te (Telugu) ...\n",
      "2023-06-15 13:44:38 INFO: File exists: /home/kapilrk04/stanza_resources/te/default.zip\n",
      "2023-06-15 13:44:40 INFO: Finished downloading models and saved to /home/kapilrk04/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# downloading stanza models for indian languages & english\n",
    "for i in [\"en\", \"hi\", \"mr\", \"ta\", \"te\"]:\n",
    "    stanza.download(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d21f6c1",
   "metadata": {},
   "source": [
    "## AWESOME-ALIGN ALIGNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad425faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from auxilaries.utils import awesomealign\n",
    "\n",
    "\n",
    "aligner = awesomealign(modelpath = 'bert-base-multilingual-cased',\n",
    "                      tokenizerpath = 'bert-base-multilingual-cased')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75dfa5c",
   "metadata": {},
   "source": [
    "## TOKENIZE AND POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd8bbfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stanza_info(text, language): #TO ADD - language parameter\n",
    "    # language accepts - en, hi, ta, te\n",
    "    nlp_lang = stanza.Pipeline(lang=language, processors='tokenize, pos')\n",
    "    doc = nlp_lang(text)\n",
    "    \n",
    "    sents, tokens, postags = [], [], []\n",
    "    \n",
    "    for sentence in doc.sentences:\n",
    "        sents.append(' '.join([f'{token.text}' for token in sentence.tokens]))\n",
    "        tokens.append([f'{token.text}' for token in sentence.words])\n",
    "        postags.append([f'{token.upos}' for token in sentence.words])\n",
    "\n",
    "    return {\"sentences\" : sents,\n",
    "           \"tokens\" : tokens,\n",
    "           \"postags\" : postags}\n",
    "\n",
    "def create_alignments_token_map(sent_src, sent_tgt, alignments):\n",
    "    \n",
    "    token_map = {}\n",
    "    sent_src = sent_src.split()\n",
    "    sent_tgt = sent_tgt.split()\n",
    "    \n",
    "    for el in alignments.split():\n",
    "        el = el.split(\"-\")\n",
    "        try:\n",
    "            token_map[sent_src[int(el[0])]] = sent_tgt[int(el[1])]\n",
    "            token_map[sent_tgt[int(el[1])]] = sent_src[int(el[0])]\n",
    "        except IndexError:\n",
    "            print(\"index error\")\n",
    "            print(sent_src, sent_tgt, alignments)\n",
    "            print(\"-\"*20)\n",
    "            token_map = None\n",
    "    \n",
    "    return token_map\n",
    "\n",
    "def get_alignment_token_map(en_sent, hi_sent):\n",
    "    '''\n",
    "    FOR FAST-ALIGN\n",
    "    line = f\"{en_sent} ||| {hi_sent}\"\n",
    "    alignments = aligner.align(line.strip())\n",
    "    '''\n",
    "    \n",
    "    #awesome-align\n",
    "    alignments = aligner.get_alignments_sentence_pair(en_sent, hi_sent)\n",
    "    token_alignment_map = create_alignments_token_map(en_sent, hi_sent, alignments)\n",
    "    return alignments, token_alignment_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f01df740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]2023-06-15 13:44:59 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50e3c1e1d994c8a8641b5244afb478b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-15 13:44:59 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "========================\n",
      "\n",
      "2023-06-15 13:44:59 INFO: Using device: cpu\n",
      "2023-06-15 13:44:59 INFO: Loading: tokenize\n",
      "2023-06-15 13:44:59 INFO: Loading: pos\n",
      "2023-06-15 13:44:59 INFO: Done loading processors!\n",
      "2023-06-15 13:44:59 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f78b39d94a524caea3abd2dcaf10b80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-15 13:45:00 INFO: Loading these models for language: hi (Hindi):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | hdtb    |\n",
      "| pos       | hdtb    |\n",
      "=======================\n",
      "\n",
      "2023-06-15 13:45:00 INFO: Using device: cpu\n",
      "2023-06-15 13:45:00 INFO: Loading: tokenize\n",
      "2023-06-15 13:45:00 INFO: Loading: pos\n",
      "2023-06-15 13:45:00 INFO: Done loading processors!\n",
      "1it [00:01,  1.51s/it]\n"
     ]
    }
   ],
   "source": [
    "# TOKENIZE-POS CODE\n",
    "\n",
    "df_translations_set = pd.read_json(\"unique_utterances_en_hi_transltions.json\")\n",
    "# df_translations_set = df_translations_set[:1]\n",
    "print(df_translations_set.shape)\n",
    "# df_translations_set.head()\n",
    "\n",
    "lang1_tokenized, lang1_pos, lang2_tokenized, lang2_pos = [], [], [], []\n",
    "\n",
    "for ind, row in tqdm(df_translations_set.iterrows()):\n",
    "    \n",
    "    languages = row.keys()   \n",
    "    lang1, lang2 = [str(lang) for lang in languages]\n",
    "\n",
    "    lang1_feats = get_stanza_info(row[lang1], lang1)\n",
    "    lang2_feats = get_stanza_info(row[lang2], lang2)\n",
    "    \n",
    "    lang1_tokenized.append(lang1_feats[\"tokens\"])\n",
    "    lang2_tokenized.append(lang2_feats[\"tokens\"])    \n",
    "    \n",
    "    lang1_pos.append(lang1_feats[\"postags\"])\n",
    "    lang2_pos.append(lang2_feats[\"postags\"])    \n",
    "\n",
    "#print(f\"en tokens : {lang1_feats['tokens']}\")\n",
    "df_translations_set[\"lang1\"] = lang1\n",
    "df_translations_set[\"lang1_tokens\"] = lang1_tokenized\n",
    "df_translations_set[\"lang1_pos\"] = lang1_pos\n",
    "\n",
    "df_translations_set[\"lang2\"] = lang2\n",
    "df_translations_set[\"lang2_tokens\"] = lang2_tokenized\n",
    "df_translations_set[\"lang2_pos\"] = lang2_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6ebb52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>hi</th>\n",
       "      <th>lang1</th>\n",
       "      <th>lang1_tokens</th>\n",
       "      <th>lang1_pos</th>\n",
       "      <th>lang2</th>\n",
       "      <th>lang2_tokens</th>\n",
       "      <th>lang2_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Okay, how much does it cost?</td>\n",
       "      <td>ठीक है, इसकी लागत कितनी है?</td>\n",
       "      <td>en</td>\n",
       "      <td>[[Okay, ,, how, much, does, it, cost, ?]]</td>\n",
       "      <td>[[INTJ, PUNCT, ADV, ADJ, AUX, PRON, VERB, PUNCT]]</td>\n",
       "      <td>hi</td>\n",
       "      <td>[[ठीक, है, ,, इसकी, लागत, कितनी, है, ?]]</td>\n",
       "      <td>[[ADJ, AUX, PUNCT, PRON, NOUN, PRON, AUX, PUNCT]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             en                           hi lang1  \\\n",
       "0  Okay, how much does it cost?  ठीक है, इसकी लागत कितनी है?    en   \n",
       "\n",
       "                                lang1_tokens  \\\n",
       "0  [[Okay, ,, how, much, does, it, cost, ?]]   \n",
       "\n",
       "                                           lang1_pos lang2  \\\n",
       "0  [[INTJ, PUNCT, ADV, ADJ, AUX, PRON, VERB, PUNCT]]    hi   \n",
       "\n",
       "                               lang2_tokens  \\\n",
       "0  [[ठीक, है, ,, इसकी, लागत, कितनी, है, ?]]   \n",
       "\n",
       "                                           lang2_pos  \n",
       "0  [[ADJ, AUX, PUNCT, PRON, NOUN, PRON, AUX, PUNCT]]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_translations_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc39bf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  9.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>hi</th>\n",
       "      <th>lang1</th>\n",
       "      <th>lang1_tokens</th>\n",
       "      <th>lang1_pos</th>\n",
       "      <th>lang2</th>\n",
       "      <th>lang2_tokens</th>\n",
       "      <th>lang2_pos</th>\n",
       "      <th>alignments_awesomealign</th>\n",
       "      <th>token_alignment_map_awesomealign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Okay, how much does it cost?</td>\n",
       "      <td>ठीक है, इसकी लागत कितनी है?</td>\n",
       "      <td>en</td>\n",
       "      <td>[[Okay, ,, how, much, does, it, cost, ?]]</td>\n",
       "      <td>[[INTJ, PUNCT, ADV, ADJ, AUX, PRON, VERB, PUNCT]]</td>\n",
       "      <td>hi</td>\n",
       "      <td>[[ठीक, है, ,, इसकी, लागत, कितनी, है, ?]]</td>\n",
       "      <td>[[ADJ, AUX, PUNCT, PRON, NOUN, PRON, AUX, PUNCT]]</td>\n",
       "      <td>[0-0 0-1 1-2 2-5 3-4 4-6 5-3 6-4 7-7 ]</td>\n",
       "      <td>[{'Okay': 'है', 'ठीक': 'Okay', 'है': 'does', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             en                           hi lang1  \\\n",
       "0  Okay, how much does it cost?  ठीक है, इसकी लागत कितनी है?    en   \n",
       "\n",
       "                                lang1_tokens  \\\n",
       "0  [[Okay, ,, how, much, does, it, cost, ?]]   \n",
       "\n",
       "                                           lang1_pos lang2  \\\n",
       "0  [[INTJ, PUNCT, ADV, ADJ, AUX, PRON, VERB, PUNCT]]    hi   \n",
       "\n",
       "                               lang2_tokens  \\\n",
       "0  [[ठीक, है, ,, इसकी, लागत, कितनी, है, ?]]   \n",
       "\n",
       "                                           lang2_pos  \\\n",
       "0  [[ADJ, AUX, PUNCT, PRON, NOUN, PRON, AUX, PUNCT]]   \n",
       "\n",
       "                  alignments_awesomealign  \\\n",
       "0  [0-0 0-1 1-2 2-5 3-4 4-6 5-3 6-4 7-7 ]   \n",
       "\n",
       "                    token_alignment_map_awesomealign  \n",
       "0  [{'Okay': 'है', 'ठीक': 'Okay', 'है': 'does', '...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_alignments, list_of_token_alignment_map = [], []\n",
    "\n",
    "for ind, row in tqdm(df_translations_set.iterrows()):\n",
    "    lang1_tokenized_sent = [\" \".join(sent_list) for sent_list in row[\"lang1_tokens\"]]\n",
    "    lang2_tokenized_sent = [\" \".join(sent_list) for sent_list in row[\"lang2_tokens\"]]\n",
    "    alignment_row, token_alignment_map_row = [], []\n",
    "    try:\n",
    "        assert len(lang1_tokenized_sent) == len(lang2_tokenized_sent)\n",
    "        for lang1sent, lang2sent in zip(lang1_tokenized_sent, lang2_tokenized_sent):\n",
    "            alignments, token_alignment_map = get_alignment_token_map(lang1sent, lang2sent)\n",
    "            alignment_row.append(alignments)\n",
    "            token_alignment_map_row.append(token_alignment_map)\n",
    "        list_of_alignments.append(alignment_row)\n",
    "        list_of_token_alignment_map.append(token_alignment_map_row)\n",
    "\n",
    "    except AssertionError:\n",
    "        alignment_row, token_alignment_map_row = None, None\n",
    "        list_of_alignments.append(None)\n",
    "        list_of_token_alignment_map.append(None)\n",
    "\n",
    "\n",
    "\n",
    "df_translations_set[\"alignments_awesomealign\"] = list_of_alignments\n",
    "df_translations_set[\"token_alignment_map_awesomealign\"] = list_of_token_alignment_map\n",
    "\n",
    "list_of_token_alignment_map\n",
    "df_translations_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac60456",
   "metadata": {},
   "source": [
    "## HEURISTIC FUNCTION FOR CODEMIX SENTENCE GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db042682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#heuristic\n",
    "def replace_noun_adj_single_aligned(sent, postags, token_map):\n",
    "    \n",
    "    codemixcandidate = \"\"\n",
    "    \n",
    "    for token, token_pos in zip(sent, postags):\n",
    "        if \"NOUN\" in token_pos or \"ADJ\" in token_pos or \"PROPN\" in token_pos:\n",
    "            if token in token_map:\n",
    "                codemixcandidate += f\" {token_map[token]}\"\n",
    "            else:\n",
    "                codemixcandidate += f\" {token}\"                \n",
    "        else:\n",
    "            codemixcandidate += f\" {token}\"\n",
    "            \n",
    "    return codemixcandidate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "16cfe8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 1920.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ठीक ADJ\n",
      "है AUX\n",
      ", PUNCT\n",
      "इसकी PRON\n",
      "लागत NOUN\n",
      "कितनी PRON\n",
      "है AUX\n",
      "? PUNCT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' Okay है , इसकी cost कितनी है ?']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_codemix_candidate(row):\n",
    "    \n",
    "    sentence = \"\"\n",
    "    for en_sent, en_pos, hi_sent, hi_pos, alignments, token_alignment_map in zip(row[\"lang1_tokens\"], row[\"lang1_pos\"], row[\"lang2_tokens\"], row[\"lang2_pos\"], row[\"alignments_awesomealign\"], row[\"token_alignment_map_awesomealign\"]):\n",
    "        sentence += replace_noun_adj_single_aligned(hi_sent, hi_pos, token_alignment_map)\n",
    "    return sentence\n",
    "\n",
    "            \n",
    "def get_codemix_candidates_for_dataframe(df):\n",
    "    codemix_candidates = []\n",
    "    \n",
    "    for ind, row in tqdm(df.iterrows()):\n",
    "        \n",
    "        cm = get_codemix_candidate(row)\n",
    "\n",
    "        codemix_candidates.append(cm)\n",
    "        \n",
    "    return codemix_candidates\n",
    "\n",
    "\n",
    "\n",
    "codemix_candidates = get_codemix_candidates_for_dataframe(df_translations_set)\n",
    "\n",
    "df_translations_set[\"codemixed-sentences\"] = codemix_candidates\n",
    "\n",
    "codemix_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27ae589",
   "metadata": {},
   "source": [
    "## OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7794f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_translations_set.to_json(\"train_unique_utterances_en_hi_transltions_token_pos_alignments.json\", \n",
    "                                          force_ascii = False, \n",
    "                                         orient = \"records\",\n",
    "                                        indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ae9af44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>hi</th>\n",
       "      <th>lang1</th>\n",
       "      <th>lang1_tokens</th>\n",
       "      <th>lang1_pos</th>\n",
       "      <th>lang2</th>\n",
       "      <th>lang2_tokens</th>\n",
       "      <th>lang2_pos</th>\n",
       "      <th>alignments_awesomealign</th>\n",
       "      <th>token_alignment_map_awesomealign</th>\n",
       "      <th>codemixed-sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Okay, how much does it cost?</td>\n",
       "      <td>ठीक है, इसकी लागत कितनी है?</td>\n",
       "      <td>en</td>\n",
       "      <td>[[Okay, ,, how, much, does, it, cost, ?]]</td>\n",
       "      <td>[[INTJ, PUNCT, ADV, ADJ, AUX, PRON, VERB, PUNCT]]</td>\n",
       "      <td>hi</td>\n",
       "      <td>[[ठीक, है, ,, इसकी, लागत, कितनी, है, ?]]</td>\n",
       "      <td>[[ADJ, AUX, PUNCT, PRON, NOUN, PRON, AUX, PUNCT]]</td>\n",
       "      <td>[0-0 0-1 1-2 2-5 3-4 4-6 5-3 6-4 7-7 ]</td>\n",
       "      <td>[{'Okay': 'है', 'ठीक': 'Okay', 'है': 'does', '...</td>\n",
       "      <td>Okay है , इसकी cost कितनी है ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             en                           hi lang1  \\\n",
       "0  Okay, how much does it cost?  ठीक है, इसकी लागत कितनी है?    en   \n",
       "\n",
       "                                lang1_tokens  \\\n",
       "0  [[Okay, ,, how, much, does, it, cost, ?]]   \n",
       "\n",
       "                                           lang1_pos lang2  \\\n",
       "0  [[INTJ, PUNCT, ADV, ADJ, AUX, PRON, VERB, PUNCT]]    hi   \n",
       "\n",
       "                               lang2_tokens  \\\n",
       "0  [[ठीक, है, ,, इसकी, लागत, कितनी, है, ?]]   \n",
       "\n",
       "                                           lang2_pos  \\\n",
       "0  [[ADJ, AUX, PUNCT, PRON, NOUN, PRON, AUX, PUNCT]]   \n",
       "\n",
       "                  alignments_awesomealign  \\\n",
       "0  [0-0 0-1 1-2 2-5 3-4 4-6 5-3 6-4 7-7 ]   \n",
       "\n",
       "                    token_alignment_map_awesomealign  \\\n",
       "0  [{'Okay': 'है', 'ठीक': 'Okay', 'है': 'does', '...   \n",
       "\n",
       "               codemixed-sentences  \n",
       "0   Okay है , इसकी cost कितनी है ?  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_translations_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eef12c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
